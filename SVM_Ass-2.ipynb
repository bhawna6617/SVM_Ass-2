{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd83f43",
   "metadata": {},
   "source": [
    "# quest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219c540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Functions: In SVMs, polynomial functions are often used as kernel functions. A polynomial kernel computes the dot product of the input feature vectors in a higher-dimensional space. The degree of the polynomial determines the complexity of the decision boundary.\n",
    "# Kernel Functions: Kernel functions, in general, are used to transform input data into a higher-dimensional space where it might be more separable. They allow the algorithm to operate efficiently in this higher-dimensional space without explicitly computing the coordinates of the data in that space. Common kernel functions include polynomial kernels, radial basis function (RBF) kernels, sigmoid kernels, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d6e6",
   "metadata": {},
   "source": [
    "# quest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4cc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can implement an SVM with a polynomial kernel in Python using Scikit-learn's svm.SVC class, which stands for Support Vector Classifier. Here's a step-by-step guide:\n",
    "\n",
    "# 1.Import Necessary Libraries:\n",
    "# from sklearn import svm\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 2. Generate Data: Generate some synthetic data for classification. You can replace this step with your own dataset.\n",
    "# Generate synthetic data\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# 3.Split Data: Split the data into training and testing sets.\n",
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4.Instantiate and Train the SVM Model:\n",
    "# Instantiate SVM classifier with polynomial kernel\n",
    "#svm_classifier = svm.SVC(kernel='poly', degree=3)  # degree is the degree of the polynomial kernel\n",
    "\n",
    "# Train the SVM classifier\n",
    "#svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 5.Make Predictions:\n",
    "# Predict labels for the test set\n",
    "#y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# 6.Evaluate Model Performance:\n",
    "# Calculate accuracy\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3194c",
   "metadata": {},
   "source": [
    "# quest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5670bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller Epsilon: When epsilon is small, the margin around the predicted function is narrow. This means that the model is more sensitive to errors and tries to fit the data points as closely as possible. As a result, the SVR model may require more support vectors to capture the intricate details of the data, especially in regions where the data points exhibit significant fluctuations or noise.\n",
    "# Larger Epsilon: Conversely, when epsilon is large, the margin around the predicted function is wider. This implies that the model allows for larger errors within this margin. As a result, the SVR model becomes less sensitive to individual data points, and it may require fewer support vectors to represent the overall trend of the data. The wider margin allows for a smoother and more generalized fit to the data, which may reduce the need for many support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7159a85",
   "metadata": {},
   "source": [
    "# quest 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0748a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sure, let's break down the effects of each parameter in Support Vector Regression (SVR) and how they affect the model's performance:\n",
    "\n",
    "# Kernel Function:\n",
    "# Linear Kernel: Suitable when the relationship between the features and target variable is approximately linear.\n",
    "# Polynomial Kernel: Useful when the relationship is non-linear and the degree parameter controls the flexibility of the model.\n",
    "# Radial Basis Function (RBF) Kernel: Effective for capturing complex non-linear relationships, with the gamma parameter determining the flexibility of the model.\n",
    "# When to Use:\n",
    "# Choose the kernel function based on the nature of the data and the complexity of the underlying relationship between features and the target variable.\n",
    "# C Parameter:\n",
    "# The C parameter controls the trade-off between the model's simplicity (smoothness) and its ability to fit the training data.\n",
    "# A smaller C leads to a smoother decision surface and allows for more errors (soft margin).\n",
    "# A larger C penalizes errors more heavily, resulting in a more complex decision surface that fits the training data closely (hard margin).\n",
    "# When to Increase or Decrease:\n",
    "# Increase C when you suspect the model is underfitting or when the training data contains outliers that should be treated as hard constraints.\n",
    "# Decrease C when you observe overfitting or when you want the model to be more tolerant of errors.\n",
    "# Epsilon Parameter:\n",
    "# Epsilon (epsilon) defines the margin of tolerance where no penalty is associated with errors.\n",
    "# It determines the width of the margin within which errors are not penalized.\n",
    "# When to Increase or Decrease:\n",
    "# Increase epsilon to allow for larger deviations from the predicted values, which can be useful when the target variable has inherent noise or uncertainty.\n",
    "# Decrease epsilon if you want to enforce a tighter fit around the training data, especially when the data is relatively noise-free and you want to minimize deviations from the observed values.\n",
    "# Gamma Parameter:\n",
    "# In the RBF kernel, gamma defines how far the influence of a single training example reaches.\n",
    "# A small gamma implies a large similarity radius, leading to a smoother decision surface.\n",
    "# A large gamma implies a smaller similarity radius, resulting in a more complex decision surface that closely fits the training data.\n",
    "# When to Increase or Decrease:\n",
    "# Increase gamma to make the model more sensitive to local variations in the data, which can be beneficial when the data is densely distributed or highly non-linear.\n",
    "# Decrease gamma to make the model less sensitive to local variations and encourage a smoother decision surface, which can prevent overfitting when the data is sparse or noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f0722",
   "metadata": {},
   "source": [
    "# quest 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing necessary libraries\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import joblib\n",
    "\n",
    "# # Load dataset\n",
    "# iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocess the data (scaling)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Create an instance of the SVC classifier\n",
    "# svc_classifier = SVC()\n",
    "\n",
    "# # Train the classifier on the training data\n",
    "# svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Use the trained classifier to predict the labels of the testing data\n",
    "# y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluate the performance of the classifier using accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # Tune hyperparameters using GridSearchCV\n",
    "# param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}\n",
    "# grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Train the tuned classifier on the entire dataset\n",
    "# tuned_classifier = grid_search.best_estimator_\n",
    "# tuned_classifier.fit(X, y)\n",
    "\n",
    "# # Save the trained classifier to a file\n",
    "# joblib.dump(tuned_classifier, 'svm_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
